#!/bin/bash
# Usage: sbatch slurm-gpu-job-script
# Prepared By: Kai Xi,  Feb 2015
#              help@massive.org.au

# NOTE: To activate a SLURM option, remove the whitespace between the '#' and 'SBATCH'

# To give your job a name, replace "MyJob" with an appropriate name
#SBATCH --job-name=isaacLee_mammoth_cifar10_2000_saved_replay_256


# To set a project account for credit charging, 
#SBATCH --account=ml20

# Request CPU resource for a serial job
#SBATCH --ntasks=1
# SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1

# Request for GPU, 
#
# Option 1: Choose any GPU whatever m2070 or K20
# Note in most cases, 'gpu:N' should match '--ntasks=N'
# SBATCH --gres=gpu:1

# Option 2: Choose GPU flavours, "k20m" or "m2070"
#SBATCH --gres=gpu:A40:1
#SBATCH --partition=gpu
# SBATCH --constraint=V100-32G
# Or
# SBATCH --gres=gpu:k20m:1

# Memory usage (MB)
#SBATCH --mem-per-cpu=120000

# Set your minimum acceptable walltime, format: day-hours:minutes:seconds
#SBATCH --time=4-00:00:00



# To receive an email when job completes or fails
#SBATCH --mail-user=ilee0022@student.monash.edu
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL


# Set the file for output (stdout)
#SBATCH --output=/home/ilee0022/ml20_scratch/ilee0022/mammoth/logs/er_cifar10_2000_saved_replay_256.out

# Set the file for error log (stderr)
#SBATCH --error=/home/ilee0022/ml20_scratch/ilee0022/mammoth/logs/er_cifar10_2000_saved_replay_256.err


# Use reserved node to run job when a node reservation is made for you already
# SBATCH --reservation=reservation_name


# Command to run a gpu job
# For example:

# deviceQuery

nvidia-smi
source /home/ilee0022/ml20_scratch/ilee0022/miniconda/bin/activate
conda activate mammoth
cd /home/ilee0022/ml20_scratch/ilee0022/mammoth
python3 utils/main.py --load_best_args --dataset seq-cifar10 --model er_buf --buffer_size 2000 --wandb_entity "safricanus66"